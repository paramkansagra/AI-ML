{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the library  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quoting = 3 meaning that we are just ignoring the double text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Restaurant_Reviews.tsv\",delimiter='\\t',quoting=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to remove the dots and the commas and also we would be removing all the lowercase and upper case letters to remove all the issues that could be created due to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # it is used to simplify the text\n",
    "import nltk # this is used for stop words i.e. the words we do not want to include like the a an and etc.\n",
    "\n",
    "# nltk.download(\"stopwords\") # if nltk is not downloaded\n",
    "\n",
    "from nltk.corpus import stopwords # importing the stop words\n",
    "from nltk.stem.porter import PorterStemmer # this is used to only use the stem words i.e. the words that are actually useful\n",
    "\n",
    "# eg. it will convert loved => love i.e. it will convert all the words into present tense\n",
    "# we would create a sparce matrix at the end and this words would be created in the sparce matrix\n",
    "# this will help us to show it\n",
    "\n",
    "corpus = [] # it will contain all the diffrent review with cleaned text\n",
    "\n",
    "for i in range(0,1000):\n",
    "    review = re.sub(\"[^a-zA-Z]\",\" \",dataset[\"Review\"][i]) # to remove all the punctation\n",
    "    review = review.lower() # convert it to lower case letter\n",
    "    # splitting the sentence to diffrent words\n",
    "    review = review.split()\n",
    "\n",
    "    allStopWords = stopwords.words(\"english\")\n",
    "    allStopWords.remove(\"not\")\n",
    "\n",
    "    ps = PorterStemmer()# porter steming\n",
    "\n",
    "    review = [ps.stem(word) for word in review if not word in set(allStopWords)]\n",
    "    review = \" \".join(review)\n",
    "\n",
    "    corpus.append(review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "y = dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[0]) # number of words that are the most frequent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into test and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size=0.2 , random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Naive Bayes model on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the model using machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(np.concatenate((y_test.reshape((len(y_test),1)) , y_pred.reshape((len(y_pred),1))),1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
